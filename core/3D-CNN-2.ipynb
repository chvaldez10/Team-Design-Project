{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"eMeES6jbzeMu","outputId":"d0224077-669b-47b2-a708-a9d6d0e36ac0"},"outputs":[{"name":"stderr","output_type":"stream","text":["Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myeneirvine\u001b[0m (\u001b[33mdetecting-respiratory-pattern\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /Users/yeneirvine/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myeneirvine\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"data":{"text/html":["Tracking run with wandb version 0.16.3"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/Users/yeneirvine/Desktop/Capstone/wandb/run-20240225_230630-dkx7szy7</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/yeneirvine/3d_cnn_project/runs/dkx7szy7' target=\"_blank\">rose-silence-2</a></strong> to <a href='https://wandb.ai/yeneirvine/3d_cnn_project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/yeneirvine/3d_cnn_project' target=\"_blank\">https://wandb.ai/yeneirvine/3d_cnn_project</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/yeneirvine/3d_cnn_project/runs/dkx7szy7' target=\"_blank\">https://wandb.ai/yeneirvine/3d_cnn_project/runs/dkx7szy7</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/yeneirvine/3d_cnn_project/runs/dkx7szy7?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x1758d4590>"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["#IMPORTS\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms, datasets\n","import os\n","from PIL import Image\n","import wandb\n","from datetime import datetime\n","\n","\n","wandb.login(key='fbe9062d8afc2237e9c82b76146a6be8f5683c2f')\n","wandb.init(project='3D-CNN', entity='detecting-respiratory-pattern')\n","\n","model_save_dir = './model_checkpoints'\n","current_time = datetime.now().strftime('%Y%m%d-%H%M%S')\n","model_filename = f'best_model_{current_time}.pth'\n","model_save_path = os.path.join(model_save_dir, model_filename)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9LbQCvFRzeMw"},"outputs":[],"source":["class BreathingDataset(Dataset):\n","    def __init__(self, root_dir, blanket_condition=None, distance=None, transform=None, max_frames=100):\n","        # 100 FRAMES are used from EACH SAMPLE. For samples with less than 100, padding is applied. This 250 number was chosen as a sort of balance between using a\n","        # lot of data but not using too much padding since that can bring about inaccuracies. Can play with this number for sure.\n","        \"\"\"\n","        Initializes the dataset.\n","\n","        :param root_dir: Base directory for the dataset (e.g., path to 'Training').\n","        :param blanket_condition: 'With Blankets' or 'Without Blankets', use None to include both.\n","        :param distance: '2 Meters' or '3 Meters', use None to include both distances.\n","        :param transform: Transformations to be applied to each image.\n","        :param max_frames: Maximum number of frames to use from each video sequence.\n","        \"\"\"\n","        self.root_dir = root_dir\n","        self.blanket_condition = blanket_condition\n","        self.distance = distance\n","        self.transform = transform\n","        self.max_frames = max_frames\n","        self.samples = []\n","\n","        conditions = ['With Blankets', 'Without Blankets'] if blanket_condition is None else [blanket_condition]\n","        distances = ['2 Meters', '3 Meters'] if distance is None else [distance]\n","\n","        for condition in conditions:\n","            for dist in distances:\n","                for label in ['Hold Breath', 'Relaxed']:\n","                    label_path = os.path.join(root_dir, condition, dist, label)\n","                    for subject_path in os.listdir(label_path):\n","                        subject_full_path = os.path.join(label_path, subject_path)\n","                        if os.path.isdir(subject_full_path):\n","                            images = sorted([img for img in os.listdir(subject_full_path) if img.endswith('.jpg')],\n","                                            key=lambda x: int(x.split('.')[0]))\n","                            # Limit to the first max_frames frames\n","                            images = images[:self.max_frames]\n","                            image_paths = [os.path.join(subject_full_path, img) for img in images]\n","                            self.samples.append((image_paths, 0 if label == 'Hold Breath' else 1))\n","\n","    def __len__(self):\n","        return len(self.samples)\n","\n","    def __getitem__(self, idx):\n","        images_path, label = self.samples[idx]\n","        images = [Image.open(img_path).convert('RGB') for img_path in images_path]\n","        # remove this potentially, since images should already be in RGB.\n","\n","        if self.transform:\n","            images = [self.transform(image) for image in images]\n","\n","        # Ensuring all sequences have the same number of frames (padding if necessary)\n","        if len(images) < self.max_frames:\n","            # Assuming the transformation to tensor has already been applied, adjust if not\n","            padding = [torch.zeros_like(images[0]) for _ in range(self.max_frames - len(images))]\n","            images += padding\n","\n","        images_stack = torch.stack(images)\n","        return images_stack, label\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cj_XTeWDzeMx"},"outputs":[],"source":["# Define transform as 224, 224 is probably wrong, but just a place holder for now.\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FatJtWpHzeMx"},"outputs":[],"source":["# THIS IS WHERE WE SPECIFY WHICH CONDITIONS / CONFIGURATIONS TO INCLUDE... Right now, its taking data WITHOUT BLANKETS and 2m and 3m (because distance = none)\n","train_dataset_without_blankets = BreathingDataset('/home/yene.irvine/rgb_10-fps/Train', blanket_condition='Without Blankets', distance=None, transform=transform)\n","val_dataset_without_blankets = BreathingDataset('/home/yene.irvine/rgb_10-fps/Validation', blanket_condition='Without Blankets', distance=None, transform=transform)\n","test_dataset_without_blankets = BreathingDataset('/home/yene.irvine/rgb_10-fps/Test', blanket_condition='Without Blankets', distance=None, transform=transform)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rVitahOozeMx"},"outputs":[],"source":["train_loader = DataLoader(train_dataset_without_blankets, batch_size=2, shuffle=True)  # Shuffle for training - this shuffles the samples (i.e. sets of frames, rather than individual frames.)\n","val_loader = DataLoader(val_dataset_without_blankets, batch_size=2, shuffle=False)  # No shuffle for validation\n","test_loader = DataLoader(test_dataset_without_blankets, batch_size=2, shuffle=False)  # No shuffle for testing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RLeAuhrezeMx"},"outputs":[],"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class Basic3DCNN(nn.Module):\n","    def __init__(self):\n","        super(Basic3DCNN, self).__init__()\n","        self.conv1 = nn.Conv3d(in_channels=3, out_channels=16, kernel_size=(3, 3, 3), stride=1, padding=1)\n","        self.pool = nn.MaxPool3d(kernel_size=2, stride=2)\n","        self.conv2 = nn.Conv3d(16, 32, kernel_size=(3, 3, 3), stride=1, padding=1)\n","        # Updated size based on input dimensions after flattening\n","        self.fc1 = nn.Linear(2508800, 512)   # Adjusted to calculated size - 32 * 56* 56 * 24\n","        # TO DO: need to refactor this large number as a variable. \n","        self.fc2 = nn.Linear(512, 2)  # Assuming 2 classes\n","\n","    def forward(self, x):\n","        x = self.pool(F.relu(self.conv1(x)))\n","        x = self.pool(F.relu(self.conv2(x)))\n","        # Adjust the flattening operation to match the updated fc1 input size\n","        x = x.view(-1, 2508800)  # Flatten the tensor for the fully connected layer \n","        x = F.relu(self.fc1(x))\n","        x = self.fc2(x)\n","        return x\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D1en5aXBzeMy","outputId":"91aefa31-98b9-4b4d-87c8-7b77631acc1f"},"outputs":[],"source":["import torch.optim as optim\n","\n","model = Basic3DCNN() # model instantiation\n","criterion = nn.CrossEntropyLoss() # loss function\n","optimizer = optim.Adam(model.parameters(), lr=0.0005) #optimizer\n","\n","# Hyperparameters\n","num_epochs = 5\n","best_val_loss = float('inf')\n","patience = 3\n","early_stop_counter = 0\n","\n","#Log the parameters (this is strictly for logging to wandb, it doesn't change anything in the model)\n","wandb.config = {\n","  \"learning_rate\": 0.0005,\n","  \"epochs\": num_epochs,\n","  \"batch_size\": 2\n","}\n","\n","wandb.watch(model, log='all')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qJcW_dJJzeMy"},"outputs":[],"source":["# Main Training Loop\n","for epoch in range(num_epochs):\n","    model.train()  # Set model to training mode\n","    running_loss = 0.0\n","\n","    for i, data in enumerate(train_loader, 0):\n","        inputs, labels = data\n","        optimizer.zero_grad()\n","        inputs = inputs.permute(0, 2, 1, 3, 4)  # Adjust dimensions\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item()\n","\n","    # Log training loss\n","    wandb.log({\"epoch\": epoch, \"train_loss\": running_loss / len(train_loader)})\n","\n","    # Validation Phase\n","    model.eval()  # Set model to evaluation mode\n","    val_loss = 0.0\n","    val_corrects = 0\n","    with torch.no_grad():\n","        for inputs, labels in val_loader:\n","            inputs = inputs.permute(0, 2, 1, 3, 4)  # Adjust dimensions\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            val_loss += loss.item()\n","            _, preds = torch.max(outputs, 1)\n","            val_corrects += torch.sum(preds == labels.data)\n","\n","    val_loss /= len(val_loader)\n","    val_accuracy = val_corrects.double() / len(val_loader.dataset)\n","    wandb.log({\"val_loss\": val_loss, \"val_accuracy\": val_accuracy})\n","\n","    print(f'Epoch {epoch+1}, Train Loss: {running_loss / len(train_loader)}, Val Loss: {val_loss}, Val Accuracy: {val_accuracy}')\n","\n","    # Early Stopping\n","    if val_loss < best_val_loss:\n","        best_val_loss = val_loss\n","        early_stop_counter = 0  # Reset counter\n","\n","        # Ensure the directory exists before saving the model\n","        os.makedirs(model_save_dir, exist_ok=True)\n","\n","        # Save the best model\n","        torch.save(model.state_dict(), model_save_path)\n","\n","    else:\n","        early_stop_counter += 1\n","        if early_stop_counter >= patience:\n","            print(\"Early stopping triggered\")\n","            break"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GQvrMjLdzeMy"},"outputs":[],"source":["# Load the best model for testing\n","model.load_state_dict(torch.load(model_save_path))\n","\n","# Testing Phase\n","test_loss = 0.0\n","test_corrects = 0\n","model.eval()  # Set model to evaluation mode\n","with torch.no_grad():\n","    for inputs, labels in test_loader:\n","        inputs = inputs.permute(0, 2, 1, 3, 4)  # Adjust dimensions\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        test_loss += loss.item()\n","        _, preds = torch.max(outputs, 1)\n","        test_corrects += torch.sum(preds == labels.data)\n","\n","test_loss /= len(test_loader)\n","test_accuracy = test_corrects.double() / len(test_loader.dataset)\n","wandb.log({\"test_loss\": test_loss, \"test_accuracy\": test_accuracy})\n","print(f'Test Loss: {test_loss}, Test Accuracy: {test_accuracy}')"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"ensf-ml","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
